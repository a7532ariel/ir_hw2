{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os \n",
    "\n",
    "loglevel = os.environ.get('LOGLEVEL', 'INFO').upper()\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "                        level=loglevel, datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "np.random.seed(17)\n",
    "torch.manual_seed(17)\n",
    "batch_size = 256\n",
    "epoch = 50\n",
    "lr = 0.01\n",
    "weight_decay = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bpr = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "users = train['UserId'].values\n",
    "num_users = len(users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_items = train['ItemId'].values\n",
    "max_item = 0\n",
    "for i, row in enumerate(positive_items):\n",
    "    \n",
    "    row_int = [int(x) for x in row.split(' ')]\n",
    "    positive_items[i] = np.array(row_int)\n",
    "    max_item = max(max_item, np.max(positive_items[i]))\n",
    "num_items = max_item+1\n",
    "\n",
    "negative_matrix = []\n",
    "all_items = np.arange(num_items)\n",
    "for i, row in enumerate(positive_items):\n",
    "    negtive_items = np.delete(all_items, row)\n",
    "    negative_matrix.append(negtive_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_users,\n",
    "                 n_items,\n",
    "                 n_factors=32,\n",
    "                 dropout_p=0.3):\n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "        self.num_users = n_users\n",
    "        self.num_items = n_items\n",
    "        \n",
    "        self.user_biases = nn.Embedding(n_users, 1)\n",
    "        self.item_biases = nn.Embedding(n_items, 1)\n",
    "        self.user_embeddings = nn.Embedding(n_users, n_factors)\n",
    "        self.item_embeddings = nn.Embedding(n_items, n_factors)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    \n",
    "    def forward(self, users, items):\n",
    "        user_embed = self.user_embeddings(users) # 1xf\n",
    "        item_embed = self.item_embeddings(items) # 1xf\n",
    "        \n",
    "        dot = (self.dropout(user_embed) * self.dropout(item_embed)).sum(dim=1, keepdim=True)\n",
    "        \n",
    "        pred = dot + self.user_biases(users) + self.item_biases(items)\n",
    "#         print(pred)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_users,\n",
    "                 n_items,\n",
    "                 n_factors=32,\n",
    "                 dropout_p=0.3,\n",
    "                 basemodel=BaseModel):\n",
    "        super(BPRModel, self).__init__()\n",
    "        self.num_users = n_users\n",
    "        self.num_items = n_items\n",
    "        self.model = basemodel(n_users, n_items, n_factors=n_factors, dropout_p=dropout_p)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, users, positives, negatives):\n",
    "        pred_positive = self.model(users, positives)\n",
    "        pred_negative = self.model(users, negatives)\n",
    "        \n",
    "        delta = pred_positive - pred_negative\n",
    "        \n",
    "        return self.sigmoid(delta)\n",
    "        \n",
    "    def predict(self, user, items):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(delta_prob):\n",
    "    bpr = torch.log(delta_prob).sum() * -1\n",
    "    return bpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.3738]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def bi_convert_data_to_feature(data, pos=1):\n",
    "    feature = []\n",
    "    for i, positive_set in enumerate(data):\n",
    "        # sample negative items\n",
    "        negative_set = np.delete(np.arrange(num_items), positive_set)\n",
    "        negative_items = np.random.choice(negative_set, size=positive_set.shape[0]*pos, replace=False)\n",
    "        \n",
    "        for j in positive_set:\n",
    "            feature.append({\n",
    "                'user': i,\n",
    "                'item': j,\n",
    "                'label': 1\n",
    "            })\n",
    "        for j in negative_items:\n",
    "            feature.append({\n",
    "                'user': i,\n",
    "                'item': j,\n",
    "                'label': 0\n",
    "            })\n",
    "        \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_convert_data_to_positive_feature(data, ratio=0.1):\n",
    "    train_feature = []\n",
    "    valid_user_positive = []\n",
    "    \n",
    "    \n",
    "    for i, positive_set in enumerate(data):\n",
    "        valid_positives = []\n",
    "        \n",
    "        x = np.arange(positive_set.shape[0])\n",
    "        np.random.shuffle(x)\n",
    "        train_positive_set = positive_set[x[int(positive_set.shape[0]*ratio):]]\n",
    "        valid_positive_set = positive_set[x[:int(positive_set.shape[0]*ratio)]]\n",
    "        for j in train_positive_set:\n",
    "            train_feature.append({\n",
    "                'user': i,\n",
    "                'positive': j,\n",
    "            })\n",
    "        for j in valid_positive_set:\n",
    "            valid_positives.append(j)\n",
    "        \n",
    "        valid_user_positive.append(valid_positives)\n",
    "    return train_feature, valid_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(users, pos=1):\n",
    "    negatives = []\n",
    "    for user in users:\n",
    "        negative_items = negative_matrix[user]\n",
    "        negatives.append(np.random.choice(negative_items, 1, replace=False)[0])\n",
    "    return torch.tensor(negatives, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature, valid_feature = bpr_convert_data_to_positive_feature(positive_items)\n",
    "\n",
    "train_users = torch.tensor([f['user'] for f in train_feature], dtype=torch.long)\n",
    "train_positives = torch.tensor([f['positive'] for f in train_feature], dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(train_users, train_positives)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle = True, batch_size=32)\n",
    "\n",
    "users_tensor = torch.tensor(users, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bpr == False: \n",
    "    model = Model(num_users, num_items)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    for ep in range(epoch):\n",
    "        tr_loss = 0\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        users, items, labels = batch\n",
    "        preds = model(users, items)\n",
    "\n",
    "        loss = criterion(preds, labels)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_map(topk, positive):\n",
    "    ap = 0\n",
    "    count = 0\n",
    "    for i, item in topk:\n",
    "        if item in positive:\n",
    "            count += 1\n",
    "            ap += count/(i+1)\n",
    "    ap /= (count + 1e-8)\n",
    "    \n",
    "    return ap\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-10 15:13:04 | INFO | Training loss: 188.3877716064453,  global step: 1\n",
      "2020-06-10 15:13:10 | INFO | Training loss: 143.49288198450108,  global step: 1001\n",
      "2020-06-10 15:13:17 | INFO | Training loss: 139.90717859949726,  global step: 2001\n",
      "2020-06-10 15:13:23 | INFO | Training loss: 136.88807656740354,  global step: 3001\n",
      "2020-06-10 15:13:30 | INFO | Training loss: 132.91760849899066,  global step: 4001\n",
      "2020-06-10 15:13:36 | INFO | Training loss: 129.6429356279623,  global step: 5001\n",
      "2020-06-10 15:13:42 | INFO | Training loss: 126.56542912873998,  global step: 6001\n",
      "2020-06-10 15:13:49 | INFO | Training loss: 123.71125918833668,  global step: 7001\n",
      "2020-06-10 15:13:55 | INFO | Training loss: 120.91747241526183,  global step: 8001\n",
      "2020-06-10 15:14:00 | INFO | Training loss: 64.82815551757812,  global step: 8756\n",
      "2020-06-10 15:14:06 | INFO | Training loss: 89.63420705385617,  global step: 9756\n",
      "2020-06-10 15:14:13 | INFO | Training loss: 86.87947035717524,  global step: 10756\n",
      "2020-06-10 15:14:19 | INFO | Training loss: 85.9614601198811,  global step: 11756\n",
      "2020-06-10 15:14:26 | INFO | Training loss: 84.01646783452605,  global step: 12756\n",
      "2020-06-10 15:14:32 | INFO | Training loss: 82.56794112101957,  global step: 13756\n",
      "2020-06-10 15:14:38 | INFO | Training loss: 81.0082786565144,  global step: 14756\n",
      "2020-06-10 15:14:45 | INFO | Training loss: 79.55982723162525,  global step: 15756\n",
      "2020-06-10 15:14:51 | INFO | Training loss: 78.10075577758667,  global step: 16756\n",
      "2020-06-10 15:14:56 | INFO | Training loss: 69.46379852294922,  global step: 17511\n",
      "2020-06-10 15:15:02 | INFO | Training loss: 61.406574078730415,  global step: 18511\n",
      "2020-06-10 15:15:09 | INFO | Training loss: 60.442818278494265,  global step: 19511\n",
      "2020-06-10 15:15:15 | INFO | Training loss: 59.75493452398827,  global step: 20511\n",
      "2020-06-10 15:15:22 | INFO | Training loss: 58.68363142585611,  global step: 21511\n",
      "2020-06-10 15:15:28 | INFO | Training loss: 57.703917078198586,  global step: 22511\n",
      "2020-06-10 15:15:34 | INFO | Training loss: 56.70821904230269,  global step: 23511\n",
      "2020-06-10 15:15:41 | INFO | Training loss: 55.93684201995878,  global step: 24511\n",
      "2020-06-10 15:15:47 | INFO | Training loss: 55.191345430526,  global step: 25511\n",
      "2020-06-10 15:15:52 | INFO | Training loss: 40.480770111083984,  global step: 26266\n",
      "2020-06-10 15:15:59 | INFO | Training loss: 45.91712804750486,  global step: 27266\n",
      "2020-06-10 15:16:05 | INFO | Training loss: 45.54505407732764,  global step: 28266\n",
      "2020-06-10 15:16:11 | INFO | Training loss: 44.706151709958576,  global step: 29266\n",
      "2020-06-10 15:16:18 | INFO | Training loss: 44.01785474751956,  global step: 30266\n",
      "2020-06-10 15:16:24 | INFO | Training loss: 43.48000042516216,  global step: 31266\n",
      "2020-06-10 15:16:30 | INFO | Training loss: 42.91543406594735,  global step: 32266\n",
      "2020-06-10 15:16:37 | INFO | Training loss: 42.51201364630002,  global step: 33266\n",
      "2020-06-10 15:16:43 | INFO | Training loss: 42.04822213154199,  global step: 34266\n",
      "2020-06-10 15:16:48 | INFO | Training loss: 31.775623321533203,  global step: 35021\n",
      "2020-06-10 15:16:55 | INFO | Training loss: 35.20582100632903,  global step: 36021\n",
      "2020-06-10 15:17:01 | INFO | Training loss: 35.17865177347087,  global step: 37021\n",
      "2020-06-10 15:17:07 | INFO | Training loss: 34.98921615495081,  global step: 38021\n",
      "2020-06-10 15:17:14 | INFO | Training loss: 34.68987047585867,  global step: 39021\n",
      "2020-06-10 15:17:20 | INFO | Training loss: 34.46765475386597,  global step: 40021\n",
      "2020-06-10 15:17:26 | INFO | Training loss: 34.17578053955157,  global step: 41021\n",
      "2020-06-10 15:17:33 | INFO | Training loss: 33.824645787609455,  global step: 42021\n",
      "2020-06-10 15:17:39 | INFO | Training loss: 33.53017946613266,  global step: 43021\n",
      "2020-06-10 15:17:44 | INFO | Training loss: 19.413232803344727,  global step: 43776\n",
      "2020-06-10 15:17:51 | INFO | Training loss: 29.73918482973859,  global step: 44776\n",
      "2020-06-10 15:17:57 | INFO | Training loss: 29.263735572914072,  global step: 45776\n",
      "2020-06-10 15:18:03 | INFO | Training loss: 29.0848186021644,  global step: 46776\n",
      "2020-06-10 15:18:10 | INFO | Training loss: 28.815118005590957,  global step: 47776\n",
      "2020-06-10 15:18:16 | INFO | Training loss: 28.65465065055646,  global step: 48776\n",
      "2020-06-10 15:18:22 | INFO | Training loss: 28.504303285746232,  global step: 49776\n",
      "2020-06-10 15:18:29 | INFO | Training loss: 28.35798331417879,  global step: 50776\n",
      "2020-06-10 15:18:35 | INFO | Training loss: 28.116183819733266,  global step: 51776\n",
      "2020-06-10 15:18:40 | INFO | Training loss: 15.404613494873047,  global step: 52531\n",
      "2020-06-10 15:18:46 | INFO | Training loss: 24.969814853591995,  global step: 53531\n",
      "2020-06-10 15:18:53 | INFO | Training loss: 25.28651813791133,  global step: 54531\n",
      "2020-06-10 15:18:59 | INFO | Training loss: 25.308978068991447,  global step: 55531\n",
      "2020-06-10 15:19:06 | INFO | Training loss: 24.996731962987226,  global step: 56531\n",
      "2020-06-10 15:19:12 | INFO | Training loss: 24.810978898618774,  global step: 57531\n",
      "2020-06-10 15:19:18 | INFO | Training loss: 24.712020648755423,  global step: 58531\n",
      "2020-06-10 15:19:25 | INFO | Training loss: 24.529205182810678,  global step: 59531\n",
      "2020-06-10 15:19:31 | INFO | Training loss: 24.439403946169346,  global step: 60531\n",
      "2020-06-10 15:19:36 | INFO | Training loss: 11.329984664916992,  global step: 61286\n",
      "2020-06-10 15:19:42 | INFO | Training loss: 22.205623326601682,  global step: 62286\n",
      "2020-06-10 15:19:49 | INFO | Training loss: 22.3054961589859,  global step: 63286\n",
      "2020-06-10 15:19:55 | INFO | Training loss: 22.243951818856427,  global step: 64286\n",
      "2020-06-10 15:20:02 | INFO | Training loss: 22.17778476349445,  global step: 65286\n",
      "2020-06-10 15:20:08 | INFO | Training loss: 22.068984216080977,  global step: 66286\n",
      "2020-06-10 15:20:14 | INFO | Training loss: 21.906286699417095,  global step: 67286\n",
      "2020-06-10 15:20:21 | INFO | Training loss: 21.809787958864653,  global step: 68286\n",
      "2020-06-10 15:20:27 | INFO | Training loss: 21.751526554321263,  global step: 69286\n",
      "2020-06-10 15:20:32 | INFO | Training loss: 31.17961883544922,  global step: 70041\n",
      "2020-06-10 15:20:38 | INFO | Training loss: 19.87628916379336,  global step: 71041\n",
      "2020-06-10 15:20:45 | INFO | Training loss: 19.967817768879023,  global step: 72041\n",
      "2020-06-10 15:20:51 | INFO | Training loss: 19.914227087829,  global step: 73041\n",
      "2020-06-10 15:20:58 | INFO | Training loss: 19.7184610972849,  global step: 74041\n",
      "2020-06-10 15:21:04 | INFO | Training loss: 19.6702372244038,  global step: 75041\n",
      "2020-06-10 15:21:10 | INFO | Training loss: 19.67517522362307,  global step: 76041\n",
      "2020-06-10 15:21:17 | INFO | Training loss: 19.62046351740657,  global step: 77041\n",
      "2020-06-10 15:21:23 | INFO | Training loss: 19.51899814279418,  global step: 78041\n",
      "2020-06-10 15:21:28 | INFO | Training loss: 22.616186141967773,  global step: 78796\n",
      "2020-06-10 15:21:34 | INFO | Training loss: 18.842022225335167,  global step: 79796\n",
      "2020-06-10 15:21:41 | INFO | Training loss: 18.49177078340484,  global step: 80796\n",
      "2020-06-10 15:21:47 | INFO | Training loss: 18.436497739218584,  global step: 81796\n",
      "2020-06-10 15:21:54 | INFO | Training loss: 18.280660255585154,  global step: 82796\n",
      "2020-06-10 15:22:00 | INFO | Training loss: 18.151686188889084,  global step: 83796\n",
      "2020-06-10 15:22:06 | INFO | Training loss: 18.13189742621174,  global step: 84796\n",
      "2020-06-10 15:22:13 | INFO | Training loss: 18.067144305854843,  global step: 85796\n",
      "2020-06-10 15:22:19 | INFO | Training loss: 18.038921336861883,  global step: 86796\n",
      "2020-06-10 15:22:24 | INFO | Training loss: 9.22831916809082,  global step: 87551\n",
      "2020-06-10 15:22:30 | INFO | Training loss: 17.102946555340566,  global step: 88551\n",
      "2020-06-10 15:22:37 | INFO | Training loss: 16.89085988799433,  global step: 89551\n",
      "2020-06-10 15:22:43 | INFO | Training loss: 16.905938237478477,  global step: 90551\n",
      "2020-06-10 15:22:50 | INFO | Training loss: 16.952926900856973,  global step: 91551\n",
      "2020-06-10 15:22:56 | INFO | Training loss: 16.946930795830504,  global step: 92551\n",
      "2020-06-10 15:23:02 | INFO | Training loss: 16.852331714418764,  global step: 93551\n",
      "2020-06-10 15:23:09 | INFO | Training loss: 16.819466841338347,  global step: 94551\n",
      "2020-06-10 15:23:15 | INFO | Training loss: 16.719898143793223,  global step: 95551\n",
      "2020-06-10 15:23:20 | INFO | Training loss: 10.077197074890137,  global step: 96306\n",
      "2020-06-10 15:23:26 | INFO | Training loss: 16.062746505518177,  global step: 97306\n",
      "2020-06-10 15:23:33 | INFO | Training loss: 15.78463218678003,  global step: 98306\n",
      "2020-06-10 15:23:39 | INFO | Training loss: 15.78691216378719,  global step: 99306\n",
      "2020-06-10 15:23:46 | INFO | Training loss: 15.710659800663915,  global step: 100306\n",
      "2020-06-10 15:23:52 | INFO | Training loss: 15.740224749225304,  global step: 101306\n",
      "2020-06-10 15:23:58 | INFO | Training loss: 15.721155202760237,  global step: 102306\n",
      "2020-06-10 15:24:05 | INFO | Training loss: 15.694460723710357,  global step: 103306\n",
      "2020-06-10 15:24:11 | INFO | Training loss: 15.719615166790112,  global step: 104306\n",
      "2020-06-10 15:24:16 | INFO | Training loss: 8.168728828430176,  global step: 105061\n",
      "2020-06-10 15:24:22 | INFO | Training loss: 14.729758964551912,  global step: 106061\n",
      "2020-06-10 15:24:29 | INFO | Training loss: 14.742464249399768,  global step: 107061\n",
      "2020-06-10 15:24:35 | INFO | Training loss: 14.760664617765034,  global step: 108061\n",
      "2020-06-10 15:24:42 | INFO | Training loss: 14.789186563440573,  global step: 109061\n",
      "2020-06-10 15:24:48 | INFO | Training loss: 14.818642531245072,  global step: 110061\n",
      "2020-06-10 15:24:54 | INFO | Training loss: 14.800585129304322,  global step: 111061\n",
      "2020-06-10 15:25:01 | INFO | Training loss: 14.782005052058429,  global step: 112061\n",
      "2020-06-10 15:27:04 | INFO | Training loss: 10.073225975036621,  global step: 131326\n",
      "2020-06-10 15:27:10 | INFO | Training loss: 13.255444175832636,  global step: 132326\n",
      "2020-06-10 15:27:17 | INFO | Training loss: 12.9658322560674,  global step: 133326\n",
      "2020-06-10 15:27:23 | INFO | Training loss: 12.982578874190462,  global step: 134326\n",
      "2020-06-10 15:27:30 | INFO | Training loss: 12.980922145534832,  global step: 135326\n",
      "2020-06-10 15:27:36 | INFO | Training loss: 13.018334806787804,  global step: 136326\n",
      "2020-06-10 15:27:42 | INFO | Training loss: 13.007655304564057,  global step: 137326\n",
      "2020-06-10 15:27:49 | INFO | Training loss: 12.973727611912402,  global step: 138326\n",
      "2020-06-10 15:27:55 | INFO | Training loss: 12.917538439939952,  global step: 139326\n",
      "2020-06-10 15:28:00 | INFO | Training loss: 8.141481399536133,  global step: 140081\n",
      "2020-06-10 15:28:06 | INFO | Training loss: 12.603678371046449,  global step: 141081\n",
      "2020-06-10 15:28:13 | INFO | Training loss: 12.703331268590311,  global step: 142081\n",
      "2020-06-10 15:28:19 | INFO | Training loss: 12.620925785541058,  global step: 143081\n",
      "2020-06-10 15:28:26 | INFO | Training loss: 12.665975219277971,  global step: 144081\n",
      "2020-06-10 15:28:32 | INFO | Training loss: 12.659794722955434,  global step: 145081\n",
      "2020-06-10 15:28:38 | INFO | Training loss: 12.57458773378014,  global step: 146081\n",
      "2020-06-10 15:28:45 | INFO | Training loss: 12.529423073996238,  global step: 147081\n",
      "2020-06-10 15:28:51 | INFO | Training loss: 12.50712059393717,  global step: 148081\n",
      "2020-06-10 15:28:56 | INFO | Training loss: 6.585021495819092,  global step: 148836\n",
      "2020-06-10 15:29:02 | INFO | Training loss: 12.431835774894243,  global step: 149836\n",
      "2020-06-10 15:29:09 | INFO | Training loss: 12.259136244811039,  global step: 150836\n",
      "2020-06-10 15:29:15 | INFO | Training loss: 12.11555338120389,  global step: 151836\n",
      "2020-06-10 15:29:22 | INFO | Training loss: 12.060580373674416,  global step: 152836\n",
      "2020-06-10 15:29:28 | INFO | Training loss: 12.005063789006687,  global step: 153836\n",
      "2020-06-10 15:29:34 | INFO | Training loss: 11.996971620418414,  global step: 154836\n",
      "2020-06-10 15:29:41 | INFO | Training loss: 11.999612034090823,  global step: 155836\n",
      "2020-06-10 15:29:47 | INFO | Training loss: 11.99546219043293,  global step: 156836\n",
      "2020-06-10 15:29:52 | INFO | Training loss: 19.858949661254883,  global step: 157591\n",
      "2020-06-10 15:29:58 | INFO | Training loss: 11.581127135784595,  global step: 158591\n",
      "2020-06-10 15:30:05 | INFO | Training loss: 11.495584260219934,  global step: 159591\n",
      "2020-06-10 15:30:11 | INFO | Training loss: 11.581157367454296,  global step: 160591\n",
      "2020-06-10 15:30:18 | INFO | Training loss: 11.55527743212255,  global step: 161591\n",
      "2020-06-10 15:30:24 | INFO | Training loss: 11.579280266736037,  global step: 162591\n",
      "2020-06-10 15:30:30 | INFO | Training loss: 11.569212570247641,  global step: 163591\n",
      "2020-06-10 15:30:37 | INFO | Training loss: 11.578192941888096,  global step: 164591\n",
      "2020-06-10 15:30:43 | INFO | Training loss: 11.6086477073278,  global step: 165591\n",
      "2020-06-10 15:30:48 | INFO | Training loss: 10.811309814453125,  global step: 166346\n",
      "2020-06-10 15:30:54 | INFO | Training loss: 11.546777146441357,  global step: 167346\n",
      "2020-06-10 15:31:01 | INFO | Training loss: 11.373747889605955,  global step: 168346\n",
      "2020-06-10 15:31:07 | INFO | Training loss: 11.253895415103026,  global step: 169346\n",
      "2020-06-10 15:31:14 | INFO | Training loss: 11.34415711155238,  global step: 170346\n",
      "2020-06-10 15:31:20 | INFO | Training loss: 11.34953094425022,  global step: 171346\n",
      "2020-06-10 15:31:26 | INFO | Training loss: 11.310389081272875,  global step: 172346\n",
      "2020-06-10 15:31:33 | INFO | Training loss: 11.320544373050891,  global step: 173346\n",
      "2020-06-10 15:31:39 | INFO | Training loss: 11.291221130089438,  global step: 174346\n",
      "2020-06-10 15:31:44 | INFO | Training loss: 15.189285278320312,  global step: 175101\n",
      "2020-06-10 15:31:50 | INFO | Training loss: 11.047244664672371,  global step: 176101\n",
      "2020-06-10 15:31:57 | INFO | Training loss: 11.098100427744807,  global step: 177101\n",
      "2020-06-10 15:32:03 | INFO | Training loss: 11.109645223347435,  global step: 178101\n",
      "2020-06-10 15:32:10 | INFO | Training loss: 11.085152185877929,  global step: 179101\n",
      "2020-06-10 15:32:16 | INFO | Training loss: 11.058017645829011,  global step: 180101\n",
      "2020-06-10 15:32:22 | INFO | Training loss: 11.044965827431287,  global step: 181101\n",
      "2020-06-10 15:32:29 | INFO | Training loss: 11.028443450894361,  global step: 182101\n",
      "2020-06-10 15:32:35 | INFO | Training loss: 10.999680694096268,  global step: 183101\n",
      "2020-06-10 15:32:40 | INFO | Training loss: 14.171843528747559,  global step: 183856\n",
      "2020-06-10 15:32:46 | INFO | Training loss: 10.768032329661267,  global step: 184856\n",
      "2020-06-10 15:32:53 | INFO | Training loss: 10.850765035368097,  global step: 185856\n",
      "2020-06-10 15:32:59 | INFO | Training loss: 10.845958274946495,  global step: 186856\n",
      "2020-06-10 15:33:05 | INFO | Training loss: 10.853983715843242,  global step: 187856\n",
      "2020-06-10 15:33:12 | INFO | Training loss: 10.858920274913562,  global step: 188856\n",
      "2020-06-10 15:33:18 | INFO | Training loss: 10.852840085364127,  global step: 189856\n",
      "2020-06-10 15:33:25 | INFO | Training loss: 10.800773918603152,  global step: 190856\n",
      "2020-06-10 15:33:31 | INFO | Training loss: 10.80651382831704,  global step: 191856\n",
      "2020-06-10 15:33:36 | INFO | Training loss: 3.5109329223632812,  global step: 192611\n",
      "2020-06-10 15:33:42 | INFO | Training loss: 10.322942160940789,  global step: 193611\n",
      "2020-06-10 15:33:49 | INFO | Training loss: 10.33830086997841,  global step: 194611\n",
      "2020-06-10 15:33:55 | INFO | Training loss: 10.404947805110712,  global step: 195611\n",
      "2020-06-10 15:34:01 | INFO | Training loss: 10.43811616978625,  global step: 196611\n",
      "2020-06-10 15:34:08 | INFO | Training loss: 10.394159923575206,  global step: 197611\n",
      "2020-06-10 15:34:14 | INFO | Training loss: 10.434452779192544,  global step: 198611\n",
      "2020-06-10 15:34:21 | INFO | Training loss: 10.460690152881792,  global step: 199611\n",
      "2020-06-10 15:34:27 | INFO | Training loss: 10.469175446869329,  global step: 200611\n",
      "2020-06-10 15:34:32 | INFO | Training loss: 16.000093460083008,  global step: 201366\n",
      "2020-06-10 15:34:38 | INFO | Training loss: 10.276956500111522,  global step: 202366\n",
      "2020-06-10 15:34:45 | INFO | Training loss: 10.27246161820232,  global step: 203366\n",
      "2020-06-10 15:34:51 | INFO | Training loss: 10.269433862723655,  global step: 204366\n",
      "2020-06-10 15:34:57 | INFO | Training loss: 10.261805064140097,  global step: 205366\n",
      "2020-06-10 15:35:04 | INFO | Training loss: 10.219812383153062,  global step: 206366\n",
      "2020-06-10 15:35:10 | INFO | Training loss: 10.25264325111712,  global step: 207366\n",
      "2020-06-10 15:35:17 | INFO | Training loss: 10.24792691646789,  global step: 208366\n",
      "2020-06-10 15:35:23 | INFO | Training loss: 10.237057621055,  global step: 209366\n",
      "2020-06-10 15:35:28 | INFO | Training loss: 17.427623748779297,  global step: 210121\n",
      "2020-06-10 15:35:34 | INFO | Training loss: 10.080202898064574,  global step: 211121\n",
      "2020-06-10 15:35:41 | INFO | Training loss: 9.89035126896038,  global step: 212121\n",
      "2020-06-10 15:35:47 | INFO | Training loss: 9.865010607882127,  global step: 213121\n",
      "2020-06-10 15:35:53 | INFO | Training loss: 9.929908883926183,  global step: 214121\n",
      "2020-06-10 15:36:00 | INFO | Training loss: 9.986859074475122,  global step: 215121\n",
      "2020-06-10 15:36:06 | INFO | Training loss: 10.031903223601248,  global step: 216121\n",
      "2020-06-10 15:36:13 | INFO | Training loss: 10.025527515508093,  global step: 217121\n",
      "2020-06-10 15:36:19 | INFO | Training loss: 10.002019277186323,  global step: 218121\n",
      "2020-06-10 15:36:24 | INFO | Training loss: 2.543973922729492,  global step: 218876\n",
      "2020-06-10 15:36:30 | INFO | Training loss: 9.81777342668661,  global step: 219876\n",
      "2020-06-10 15:36:37 | INFO | Training loss: 9.819119974888903,  global step: 220876\n",
      "2020-06-10 15:36:43 | INFO | Training loss: 9.863512252220032,  global step: 221876\n",
      "2020-06-10 15:36:49 | INFO | Training loss: 9.804257322031091,  global step: 222876\n",
      "2020-06-10 15:36:56 | INFO | Training loss: 9.76857227495827,  global step: 223876\n",
      "2020-06-10 15:37:02 | INFO | Training loss: 9.806034011237722,  global step: 224876\n",
      "2020-06-10 15:37:09 | INFO | Training loss: 9.782162487566463,  global step: 225876\n",
      "2020-06-10 15:37:15 | INFO | Training loss: 9.78149943596988,  global step: 226876\n",
      "2020-06-10 15:37:20 | INFO | Training loss: 4.494019508361816,  global step: 227631\n",
      "2020-06-10 15:37:26 | INFO | Training loss: 9.562734329974377,  global step: 228631\n",
      "2020-06-10 15:37:33 | INFO | Training loss: 9.608150746213502,  global step: 229631\n",
      "2020-06-10 15:37:39 | INFO | Training loss: 9.570765392576126,  global step: 230631\n",
      "2020-06-10 15:37:45 | INFO | Training loss: 9.535154956559841,  global step: 231631\n",
      "2020-06-10 15:37:52 | INFO | Training loss: 9.583945732966729,  global step: 232631\n",
      "2020-06-10 15:37:58 | INFO | Training loss: 9.571240420014515,  global step: 233631\n",
      "2020-06-10 15:38:05 | INFO | Training loss: 9.5653654401208,  global step: 234631\n",
      "2020-06-10 15:38:11 | INFO | Training loss: 9.578927328230186,  global step: 235631\n",
      "2020-06-10 15:38:16 | INFO | Training loss: 14.92339038848877,  global step: 236386\n",
      "2020-06-10 15:38:22 | INFO | Training loss: 9.447424223492076,  global step: 237386\n",
      "2020-06-10 15:38:29 | INFO | Training loss: 9.506011007250338,  global step: 238386\n",
      "2020-06-10 15:38:35 | INFO | Training loss: 9.425629735707998,  global step: 239386\n",
      "2020-06-10 15:38:41 | INFO | Training loss: 9.435484334457758,  global step: 240386\n",
      "2020-06-10 15:38:48 | INFO | Training loss: 9.454754428156994,  global step: 241386\n",
      "2020-06-10 15:38:54 | INFO | Training loss: 9.397598876394524,  global step: 242386\n",
      "2020-06-10 15:39:00 | INFO | Training loss: 9.39661279556973,  global step: 243386\n",
      "2020-06-10 15:39:07 | INFO | Training loss: 9.395472029256398,  global step: 244386\n",
      "2020-06-10 15:39:12 | INFO | Training loss: 2.4392504692077637,  global step: 245141\n",
      "2020-06-10 15:39:18 | INFO | Training loss: 9.17635489772488,  global step: 246141\n",
      "2020-06-10 15:39:24 | INFO | Training loss: 9.309456507662784,  global step: 247141\n",
      "2020-06-10 15:39:31 | INFO | Training loss: 9.260079126205495,  global step: 248141\n",
      "2020-06-10 15:39:37 | INFO | Training loss: 9.317694237904977,  global step: 249141\n",
      "2020-06-10 15:39:44 | INFO | Training loss: 9.304602925192473,  global step: 250141\n",
      "2020-06-10 15:39:50 | INFO | Training loss: 9.289084013989122,  global step: 251141\n",
      "2020-06-10 15:39:56 | INFO | Training loss: 9.284768370164665,  global step: 252141\n",
      "2020-06-10 15:40:03 | INFO | Training loss: 9.276652149804875,  global step: 253141\n",
      "2020-06-10 15:40:08 | INFO | Training loss: 11.419294357299805,  global step: 253896\n",
      "2020-06-10 15:40:14 | INFO | Training loss: 9.164877148298594,  global step: 254896\n",
      "2020-06-10 15:40:20 | INFO | Training loss: 9.161065225420089,  global step: 255896\n",
      "2020-06-10 15:40:27 | INFO | Training loss: 9.137999442409413,  global step: 256896\n",
      "2020-06-10 15:40:33 | INFO | Training loss: 9.10082819389719,  global step: 257896\n",
      "2020-06-10 15:40:40 | INFO | Training loss: 9.040940981606344,  global step: 258896\n",
      "2020-06-10 15:40:46 | INFO | Training loss: 9.026473426119603,  global step: 259896\n",
      "2020-06-10 15:40:52 | INFO | Training loss: 8.997464360892474,  global step: 260896\n",
      "2020-06-10 15:40:59 | INFO | Training loss: 8.990852836682906,  global step: 261896\n"
     ]
    }
   ],
   "source": [
    "if bpr:\n",
    "    model = BPRModel(num_users, num_items)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=lr, weight_decay=0)\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    max_map = 0\n",
    "    for ep in range(epoch):\n",
    "        \n",
    "        tr_loss = 0\n",
    "        tr_step = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            model.zero_grad()\n",
    "            tr_step += 1\n",
    "            global_step += 1\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            users, positives = batch\n",
    "            negatives = negative_sampling(users)\n",
    "            output = model(users, positives, negatives)\n",
    "\n",
    "            loss = bpr_loss(output)\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if step%1000 == 0:\n",
    "                logging.info(f'Training loss: {tr_loss/tr_step},  global step: {global_step}')\n",
    "\n",
    "    \n",
    "        # valid \n",
    "        model.eval()\n",
    "        k = 50\n",
    "        avg_map = 0\n",
    "        for user in users_tensor:\n",
    "            scores = []\n",
    "            candidate_items = valid_feature[user] + negative_matrix[user]\n",
    "            for candidate in candidate_items:\n",
    "                score = model.predict(user, candidate)\n",
    "                scores.append(score)\n",
    "            scores = np.array(scores)\n",
    "            topk_indices = scores.argsort()[-k:][::-1]\n",
    "            map_score = calc_map(topk_indices, valid_feature[user])\n",
    "            avg_map += map_score\n",
    "        \n",
    "        avg_map /= num_users    \n",
    "        logging.info(f'EPOCH: {ep}, Valid MAP: {avg_map}')\n",
    "        \n",
    "        if avg_map > max_map:\n",
    "            max_map = avg_map\n",
    "            torch.save(model, f'ckpt_{avg_map}.model')\n",
    "            logging.info(f'saving model with valid map {avg_map}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
